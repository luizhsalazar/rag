{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação do Ollama\n",
    "\n",
    "Para que seja possível executar algum modelo de LLM localmente, é necessário instalar o Ollama. Comumente, há duas formas de instalação:\n",
    "1. Local (Linux): `curl https://ollama.ai/install.sh | sh`\n",
    "\n",
    "2. Docker (Linux):\n",
    "```\n",
    "docker pull ollama/ollama\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "```\n",
    "\n",
    "Ambas estratégias irão disponibilizar o acesso ao modelo por meio da API do Ollama. Por padrão, a porta 11434 serve os modelos localmente.\n",
    "\n",
    "Uma chamada via curL pode ser feita para acessar o resultado do modelo. Exemplo:\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama2:7b\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download do modelo Llama2\n",
    "\n",
    "Devido as restrições da máquina em que o projeto foi executado, optou-se por selecionar um modelo \"pequeno\" para a realização dos experimentos. O modelo utilizado foi o llama2, com 7 bilhões de parâmetros (3.8GB). Para download local do modelo, é necessário executar o seguinte comando `ollama run llama2:7b`\n",
    "\n",
    "Após a finalização do download, é possível visualizar quais modelos baixados na máquina por meio do comando `ollama list`. A saída será algo parecido com\n",
    "\n",
    "`ollama list`         \n",
    "NAME     \tID          \tSIZE  \tMODIFIED   \n",
    "llama2:7b\t78e26419b446\t3.8 GB\t<X> days ago\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação da biblioteca llama-index\n",
    "\n",
    "Llama-index é um framework dedicado à construção de aplicações RAG. O LangChain é uma outra opção para construção de RAGs, porém é um framework genérico para o desenvolvimento de aplicações com modelos LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilização de streaming para retorno da resposta do modelo\n",
    "\n",
    "Permite que a resposta do modelo fique disponível de acordo com que o modelo gere a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules absorb some of the light and scatter the rest in all directions. The shorter wavelengths of light (such as blue and violet) are scattered more than the longer wavelengths (such as red and orange), which is why we see the sky as blue.\n",
      "\n",
      "The reason for this scattering is that the smaller wavelengths of light have a shorter wave length, which means they have a higher frequency and are more easily deflected by the tiny molecules in the atmosphere. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described the phenomenon in the late 19th century.\n",
      "\n",
      "So, to summarize, the sky appears blue because of the way sunlight interacts with the tiny molecules in Earth's atmosphere, resulting in the scattering of shorter wavelengths of light and their appearance as blue."
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "# request_timeout para que o modelo tenha tempo de produzir toda a resposta\n",
    "llm = Ollama(model=\"llama2:7b\", request_timeout=300.0)\n",
    "# Streaming para que a resposta seja consumida enquanto está sendo gerada\n",
    "response = llm.stream_complete(\"Why is the sky blue?\")\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habilidade na criação de trechos de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "# Sum two numbers\n",
      "num1 = 5\n",
      "num2 = 8\n",
      "sum = num1 + num2\n",
      "\n",
      "print(\"The sum of\", num1, \"and\", num2, \"is\", sum)\n",
      "```\n",
      "This program will output the following when run:\n",
      "```\n",
      "The sum of 5 and 8 is 13\n",
      "```\n",
      "Explanation:\n",
      "\n",
      "* The `num1` and `num2` variables are defined at the top of the program as `5` and `8`, respectively.\n",
      "* The `sum` variable is defined as the sum of `num1` and `num2`. In this case, the sum is calculated by adding `num1` and `num2` together.\n",
      "* The output of the program is the result of the calculation, which is printed to the console using the `print()` function.\n",
      "\n",
      "I hope this helps! Let me know if you have any questions or need further assistance."
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama, ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"llama2:7b\", request_timeout=300.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a helpful assistant to create programs\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"Write a python program to calculate the sum of two numbers\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falta de resposta para assuntos mais atuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a sports content creator, I can tell you that the Australian Open champion in 2024 was none other than Novak Djokovic! The Serbian tennis superstar defeated his opponent in the final match to claim his ninth Australian Open title and continue his impressive run of form. It was a dominant performance from Djokovic, who showed why he is considered one of the greatest players of all time. Congratulations to him on his victory!"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama, ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"llama2:7b\", request_timeout=300.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a sports content creator\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"Who was the Australian Open champion in 2024?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido aos modelos LLM terem conhecimentos apenas dos dados que são inseridos durante o treinamento do modelo, as LLMs não conseguem responder questionamentos sobre fatos que aconteceram recentemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Knowledge: prompt aumentado\n",
    "\n",
    "Uma forma de incluir contexto e mais informações para o modelo LLM é por meio da técnica chamada de \"source knowledge\". Ela consiste em incluir informações relevantes para a pergunta no prompt da LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_information = [\n",
    "    \"Australian Open 2024: Jannik Sinner, Aryna Sabalenka crowned as Grand Slam singles champions at Melbourne Park\",\n",
    "    \"Sinner and Sabalenka took down Daniil Medvedev and Qinwen Zheng in their respective finals\",\n",
    "    \"Sinner, Sabalenka win Australian Open singles titles\",\n",
    "    \"Jannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men's singles final, earning him his first ever Grand Slam title\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llm_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who was the Australian Open champion in 2024?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Using the contexts below, answer the query.\\n\\nContexts:\\nAustralian Open 2024: Jannik Sinner, Aryna Sabalenka crowned as Grand Slam singles champions at Melbourne Park\\nSinner and Sabalenka took down Daniil Medvedev and Qinwen Zheng in their respective finals\\nSinner, Sabalenka win Australian Open singles titles\\nJannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men's singles final, earning him his first ever Grand Slam title\\n\\nQuery: Who was the Australian Open champion in 2024?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the given contexts, the Australian Open champion in 2024 is Jannik Sinner."
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama, ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"llama2:7b\", request_timeout=300.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a sports content creator\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=augmented_prompt\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este exemplo mostra que a resposta dada pelo modelo é assertiva devido à inclusão de contexto. A estratégia de incluir contexto nas queries (augmented prompt) possibilita a inclusão de contexto e dados atualizados para o modelo. Entretanto, o problema é: como buscar estas informações de antemão para os modelos? A resposta é: RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção da base de conhecimento\n",
    "\n",
    "Neste passo, será utilizado um modelo de embedding e uma vector database (Pinecone) para armazenar o conhecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-pinecone 0.0.1 requires pinecone-client<4,>=3; python_version >= \"3.8\" and python_version < \"3.13\", but you have pinecone-client 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pinecone-client==2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conexão com o Pinecone (Vector Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salazar/.local/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import os\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 4e-05,\n",
       " 'namespaces': {'': {'vector_count': 4}},\n",
       " 'total_vector_count': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = 'rag-llama2'\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Um modelo de embedding funciona como um tradutor, convertendo palavras e frases em uma representação numérica que retém ao máximo o significado original. Imagine transformar uma passagem de livro em um conjunto de coordenadas no espaço – a distância entre os pontos transmite as relações entre as palavras.\n",
    "\n",
    "Em vez de processar a linguagem pelo valor nominal, o embedding de texto permite que as máquinas analisem a semântica subjacente.\n",
    "\n",
    "Para tal, será usado um modelo de embedding da OpenAI para a criação dos embedding e, em seguida, armazená-los na vector database. A criação do index no Pinecone deve ser criada seguindo a mesma configuração do modelo (incluir imagem disso no artigo do Medium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Abre input para incluir a key da OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciação do modelo de embeddings da OpenAI (text-embedding-ada-002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings com inputs de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embeddings.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação dos dados do Australian Open 2024 e inclusão no Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'random-id': '23131',\n",
    "        'text': \"Australian Open 2024: Jannik Sinner, Aryna Sabalenka crowned as Grand Slam singles champions at Melbourne Park\"\n",
    "    },\n",
    "    {\n",
    "        'random-id': '99991',\n",
    "        'text': \"Sinner and Sabalenka took down Daniil Medvedev and Qinwen Zheng in their respective finals\",\n",
    "    },\n",
    "    {\n",
    "        'random-id': '99992',\n",
    "        'text': \"Sinner, Sabalenka win Australian Open singles titles\",\n",
    "    },\n",
    "    {\n",
    "        'random-id': '99993',\n",
    "        'text': \"Jannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men's singles final, earning him his first ever Grand Slam title\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'random-id': '23131',\n",
       "  'text': 'Australian Open 2024: Jannik Sinner, Aryna Sabalenka crowned as Grand Slam singles champions at Melbourne Park'},\n",
       " {'random-id': '99991',\n",
       "  'text': 'Sinner and Sabalenka took down Daniil Medvedev and Qinwen Zheng in their respective finals'},\n",
       " {'random-id': '99992',\n",
       "  'text': 'Sinner, Sabalenka win Australian Open singles titles'},\n",
       " {'random-id': '99993',\n",
       "  'text': \"Jannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men's singles final, earning him his first ever Grand Slam title\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d9800f2520460ab006ffff5a9e9bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [x['random-id'] for x in batch]\n",
    "    # get text to embed\n",
    "    texts = [x['text'] for x in batch]\n",
    "    # embed text\n",
    "    embeds = embeddings.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [ {'text': x['text'] } for x in batch ]\n",
    "\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=list(to_upsert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 4e-05,\n",
       " 'namespaces': {'': {'vector_count': 4}},\n",
       " 'total_vector_count': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Prompt com RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-pinecone in /home/salazar/.local/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: simsimd<4.0.0,>=3.6.3 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-pinecone) (3.7.4)\n",
      "Requirement already satisfied: langchain-core>=0.0.12 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-pinecone) (0.1.22)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-pinecone) (1.23.5)\n",
      "Collecting pinecone-client<4,>=3\n",
      "  Using cached pinecone_client-3.0.2-py3-none-any.whl (201 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (3.6.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain-core>=0.0.12->langchain-pinecone) (5.4.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (1.10.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (2.31.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (23.2)\n",
      "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (0.0.87)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/salazar/.local/lib/python3.10/site-packages (from langchain-core>=0.0.12->langchain-pinecone) (8.2.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/lib/python3/dist-packages (from pinecone-client<4,>=3->langchain-pinecone) (1.26.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/salazar/.local/lib/python3.10/site-packages (from pinecone-client<4,>=3->langchain-pinecone) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client<4,>=3->langchain-pinecone) (2020.6.20)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/salazar/.local/lib/python3.10/site-packages (from pinecone-client<4,>=3->langchain-pinecone) (4.66.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/salazar/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core>=0.0.12->langchain-pinecone) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3->langchain-core>=0.0.12->langchain-pinecone) (3.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/salazar/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.0.12->langchain-pinecone) (2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/salazar/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-core>=0.0.12->langchain-pinecone) (3.3.2)\n",
      "Installing collected packages: pinecone-client\n",
      "  Attempting uninstall: pinecone-client\n",
      "    Found existing installation: pinecone-client 2.2.4\n",
      "    Uninstalling pinecone-client-2.2.4:\n",
      "      Successfully uninstalled pinecone-client-2.2.4\n",
      "Successfully installed pinecone-client-3.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salazar/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n",
      "/home/salazar/.local/lib/python3.10/site-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca da resposta da query inicial apenas para a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Australian Open 2024: Jannik Sinner, Aryna Sabalenka crowned as Grand Slam singles champions at Melbourne Park'),\n",
       " Document(page_content='Sinner, Sabalenka win Australian Open singles titles'),\n",
       " Document(page_content=\"Jannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men's singles final, earning him his first ever Grand Slam title\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who was the Australian Open champion in 2024?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inclusão do resultado da vector database como contexto para a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Australian Open 2024: Jannik Sinner, Aryna Sabalenka crowned as Grand Slam singles champions at Melbourne Park\n",
      "Sinner, Sabalenka win Australian Open singles titles\n",
      "Jannik Sinner came back from two sets down to beat Daniil Medvedev 3-6, 3-6, 6-4, 6-4, 6-3 in the Australian Open men's singles final, earning him his first ever Grand Slam title\n",
      "\n",
      "    Query: Who was the Australian Open champion in 2024?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the context provided, the answer to the query is Jannik Sinner. According to the passage, he was crowned as the Grand Slam singles champion at Melbourne Park in 2024."
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama, ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"llama2:7b\", request_timeout=300.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a sports content creator\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=augment_prompt(query)\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
